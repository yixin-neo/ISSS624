---
title: "Hands-on_Ex05"
editor: visual
---

# 5  Geographical Segmentation with Spatially Constrained Clustering Techniques

## 5.1 Overview

In this hands-on exercise, I will

-   delineate homogeneous region by using geographically referenced multivariate data

-   There are two major analysis, namely:

    -   hierarchical cluster analysis; and

    -   spatially constrained cluster analysis.

### 5.1.1 Learning Outcome

-   to convert GIS polygon data into R's simple feature data.frame by using appropriate functions of **sf** package of R;

-   to convert simple feature data.frame into R's SpatialPolygonDataFrame object by using appropriate **sf** of package of R;

-   to perform custer analysis by using *hclust()* of Base R;

-   to perform spatially constrained cluster analysis using *skater()* of Base R; and

-   to visualise the analysis output by using **ggplot2** and **tmap** package.

## 

5.2 Getting Started

### 5.2.1 The analytical question

In this hands-on exercise, we are interested to delineate [Shan State](https://en.wikipedia.org/wiki/Shan_State), [Myanmar](https://en.wikipedia.org/wiki/Myanmar) into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

## 5.3 The data

Two data sets will be used in this study. They are:

-   Myanmar Township Boundary Data (i.e. *myanmar_township_boundaries*) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features. \<- geospatial data

-   *Shan-ICT.csv*: This is an extract of [**The 2014 Myanmar Population and Housing Census Myanmar**](https://myanmar.unfpa.org/en/publications/2014-population-and-housing-census-myanmar-data-sheet) at the township level. \< aspatial data

Both data sets are download from [Myanmar Information Management Unit (MIMU)](http://themimu.info/)

### 5.3.1 Installing and loading R packages

```{r}
pacman::p_load(rgdal, spdep, tmap, sf, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse)
```

## 5.4 Data Import and Prepatation

### 5.4.1 Importing geospatial data into R environment

The Myanmar Township Boundary GIS data is in ESRI shapefile format. It will be imported into R environment by using the [*st_read()*](https://www.rdocumentation.org/packages/sf/versions/0.7-2/topics/st_read) function of **sf**. Opening the .prj file in notepad reveals that it is in WGS 84 which has a crs code of 4326.

```{r}
shan_sf <-st_read(dsn='data/geospatial',
                  layer='myanmar_township_boundaries')
```

Check the states in Myanmar:

```{r}
library(funModeling)
freq (data = shan_sf,
input = 'ST')
```

```{r}
#| eval: false
dplyr::count(wp_nga, status_cle, sort = TRUE)
```

Retrieve only the SHAN states

```{r}
shan_sf <- shan_sf %>% 
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)"))
```

Checking after filtering, Reduced to only 55 Rows, exactly the same as the number of rows in excel ICT file.

```{r}
shan_sf
```

Notice that sf.data.frame is conformed to Hardy Wickham's [tidy](https://edzer.github.io/rstudio_conf/#1) framework.

Since *shan_sf* is conformed to tidy framework, we can also *glimpse()* to reveal the data type of it's fields.

```{r}
glimpse(shan_sf)
```

### 5.4.2 Importing aspatial data into R environment

The csv file will be import using *read_csv* function of **readr** package.

```{r}
ict <- read_csv('data/aspatial/Shan-ICT.csv')
```

The imported InfoComm variables are extracted from **The 2014 Myanmar Population and Housing Census Myanmar**. The attribute data set is called *ict*. It is saved in R's \* tibble data.frame\* format.

The code chunk below reveal the summary statistics of *ict* data.frame.

```{r}
class(ict)
```

```{r}
summary(ict)
```

There are a total of eleven fields and 55 observation in the tibble data.frame. In Myanmar, State - district - township

### 5.4.3 Derive new variables using **dplyr** package

The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.

In order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.

```{r}

ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

Six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.

```{r}
summary(ict_derived)
```

## 5.5 Exploratory Data Analysis (EDA)

### 5.5.1 EDA using statistical graphics

We can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

Histogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)

```{r}
ggplot(data=ict_derived,
       aes(x=`RADIO`)) +
  geom_histogram(bins=20,
                 color='black',
                 fill='light blue')
```

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data=ict_derived,
       aes(x=`RADIO`)) +
  geom_boxplot(color='black',
                 fill='light blue')
```

Next, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.

```{r}
ggplot(data=ict_derived,
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20,
                 color='black',
                 fill='light blue')
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

RADIO_PR (RADIO/TOTAL \* 1000) is a better variable to use than RADIO alone as its is lesser in range, more normally distributed and less outlier.

To create multiple histograms of the other variables in a single plot:

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Next, the [*ggarange()*](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html) function of [**ggpubr**](https://rpkgs.datanovia.com/ggpubr/) package is used to group these histograms together.

```{r}
ggarrange(radio, tv ,llphone, mphone, computer, internet,
          ncol=3,
          nrow=2)
```

### 5.5.2 EDA using choropleth map

#### 5.5.2.1 Joining geospatial data with aspatial data

-   First, we need to combine the geospatial data (i.e. *shan_sf*) and the aspatial data (i.e. *ict_derived*) together using the [*left_join*](https://dplyr.tidyverse.org/reference/join.tbl_df.html) function of **dplyr** package.

-   The *shan_sf* simple feature data.frame will be used as the base data object and the *ict_derived* data.frame will be used as the join table.

```{r}
shan_sf <- left_join(shan_sf,
                     ict_derived,
                     by = c('TS_PCODE'='TS_PCODE'))
```

The message above shows that *TS_CODE* field is the common field used to perform the left-join.

It is important to note that there is no new output data been created. Instead, the data fields from *ict_derived* data frame are now updated into the data frame of *shan_sf*.

#### 5.5.2.2 Preparing a choropleth map

To have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.

The code chunks below are used to prepare the choroplethby using the *qtm()* function of **tmap** package.

```{r}
qtm(shan_sf, 'RADIO_PR') +
  tm_layout(legend.width= 0.3,
            legend.height = 0.3)
```

In order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) +
  tm_fill(col='TT_HOUSEHOLDS',
          n= 5,
          style='jenks',
          title = 'Total households') +
  tm_borders(alpha = 0.5) +
  tm_layout(legend.width= 0.3,
            legend.height = 0.3)

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) +
  tm_layout(legend.width= 0.3,
            legend.height = 0.3)

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

Notice that the choropleth maps above clearly show that townships with relatively larger number of households are also showing relatively higher number of radio ownership.

Now let us plot the choropleth maps showing the distribution of total number of households and Radio penetration rate by using the code chunk below.

```{r}
#tmap_mode('plot')
```

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

There are some regions with lower number of households that actually have higher RADIO_PR.

## 5.6 Correlation Analysis

Remember for hierarchical clustering, there are three conditions to be met: (1) not to large a range (else standardisation) , (2) no missing values and (3) no multi-collinearity in the data variables.

Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.

In this section, you will learn how to use [*corrplot.mixed()*](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) function of [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) package to visualise and analyse the correlation of the input variables.

```{r}
attributes(ict_derived)
```

Get the correlationship between selected columns 12 to 17

```{r}
cluster_vars.cor = cor(ict_derived[12:17])
cluster_vars.cor
```

Plot the correlationship scatterlot

| **corrplot.mixed** Using mixed methods to visualize a correlation matrix. Description: Using mixed methods to visualize a correlation matrix.

corrplot.mixed( corr, lower = "number", upper = "circle", tl.pos = c("d", "lt", "n"), diag = c("n", "l", "u"), bg = "white", addgrid.col = "grey", lower.col = NULL, upper.col = NULL, plotCI = c("n", "square", "circle", "rect"), mar = c(0, 0, 0, 0), \... ) \|

**Arguments**

**corr** Matrix, the correlation matrix to visualize.

**lower** Character, the visualization method for the lower triangular correlation matrix.

**upper** Character, the visualization method for the upper triangular correlation matrix.

**tl.pos** Character, 'lt', 'd' or 'n', giving position of text labels, 'lt' means left and top, 'd' means diagonal. If 'n', add no textlabel.

**diag** Character, for specifying the glyph on the principal diagonal. It is one of 'n' (default, draw nothing), 'l' (draw the glyphs of lower triangular) or 'u' (draw the glyphs of upper triangular).

**bg** The background color.

**addgrid.col** See the addgrid.col parameter in the function corrplot

**lower.col** Passed as col parameter to the lower matrix.

**upper.col** Passed as col parameter to the upper matrix.

**plotCI** See the plotCI parameter in the function corrplot

```{r}
corrplot.mixed(cluster_vars.cor,
               lower ='ellipse',
               upper= 'number',
               tl.pos = 'lt',
               diag = 'l',
               tl.col='black')
```

if the ellipse is very thin, correlation is strong. the direction of the ellipse tells us the sign of correlation.

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both

## 5.7 Hierarchy Cluster Analysis

In this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:

### 5.7.1 Extracting clustering variables

The code chunk below will be used to extract the clustering variables from the *shan_sf* simple feature object into data.frame. The INTERNET_PR is intentionally left out as it is highly correlated with COMPUTER_PR

**st_set_geometry(NULL)** will drop away the **geometry column.**

select() -ing only the clustering variable will not remove the geometry column.

```{r}
cluster_vars <- shan_sf %>% 
  st_set_geometry(NULL) %>% 
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars, 10)
```

TAKE NOTE: input of hclust() function must strictly contain only the [clustering variables]{.underline} we need. Cannot have township column 'TS.x'. Is it more for the dist() function used to compute the proximity matrix instead?

Next, we need to change the rows ID by township name instead of row number by using the code chunk below.

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars, 10)
```

Notice that the row number has been replaced into the township name.

Now, we will delete the TS.x field by using the code chunk below.

Do not select column 1 as it is TS.x

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

### 5.7.2 Data Standardisation

In general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.

### 5.7.3 Min-Max standardisation

In the code chunk below, *normalize()* of [*heatmaply*](https://cran.r-project.org/web/packages/heatmaply/) package is used to standardise the clustering variables by using Min-Max method. The *summary()* is then used to display the summary statistics of the standardised clustering variables.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

The values range of the Min-max standardised clustering variables are 0-1 now.

### 5.7.4 Z-score standardisation

Z-score standardisation can be performed easily by using [*scale()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) of Base R. The code chunk below will be used to standardisation the clustering variables by using Z-score method.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

It is interesting to note that shan_ict.z is a matrix array and not a data frame

```{r}
class(shan_ict)
class(shan_ict.z)
```

Notice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.

**Note:** [*describe()*](https://www.rdocumentation.org/packages/Hmisc/versions/4.4-0/topics/describe) of [**psych**](https://cran.r-project.org/web/packages/psych/) package is used here instead of *summary()* of Base R because the earlier provides standard deviation.

### 5.7.5 Visualising the standardised clustering variables

Beside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.

The code chunk below plot the scaled *Radio_PR* field.

```{r}
r <- ggplot(data = ict_derived,
            aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color = 'black',
                 fill = 'light green')

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot (data= shan_ict_s_df, aes(x=`RADIO_PR`)) +
    geom_histogram(bins=20, 
                 color = 'black',
                 fill = 'light green') +
  ggtitle('Min-Max standardisation')

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot (data= shan_ict_z_df, aes(x=`RADIO_PR`)) +
    geom_histogram(bins=20, 
                 color = 'black',
                 fill = 'light green') +
  ggtitle('Z-score standardisation')

ggarrange (r,s,z,
          ncol = 3, 
          nrow = 1)
  
```

Notice that the overall distribution of the clustering variables will change after the data standardisation. Hence, it is advisible **NOT** to perform data standardisation if the values range of the clustering variables are not very large.

```{r}
r <- ggplot(data=ict_derived,
            aes(x = `RADIO_PR`)) +
  geom_density(color= 'black',
            fill='light blue') +
  ggtitle("Raw values without standardistation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df,
            aes(x = `RADIO_PR`)) +
  geom_density(color= 'black',
            fill='light blue') +
  ggtitle("Min_Max standardistation")


shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot (data= shan_ict_z_df, aes(x=`RADIO_PR`)) +
  geom_density(color= 'black',
            fill='light blue') +
  ggtitle("Z-score standardistation")

ggarrange(r, s,z,
          ncol= 3,
          nrow=1)
  
```

### 5.7.6 Computing proximity matrix

In R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using [*dist()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/dist.html) of R.

*dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix.

The code chunk below is used to compute the proximity matrix using *euclidean* method.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
proxmat
```

```{r}
class(proxmat)
```

### 5.7.7 Computing hierarchical clustering

In R, there are several packages provide hierarchical clustering function. In this hands-on exercise, [*hclust()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html) of R stats will be used.

*hclust()* employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: [**ward.D**]{.underline}**, [ward.D2]{.underline}, [single]{.underline}, [complete]{.underline}, [average]{.underline}([UPGMA]{.underline}), [mcquitty]{.underline}([WPGMA]{.underline}), [median(WPGMC]{.underline}) and [centroid(UPGMC)]{.underline}.**

The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class **hclust** which describes the tree produced by the clustering process.

hclust() needs two inputs: **proximty matrix** and **metho**d **for the hierachical clustering** that we are using. It is important to note that

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

```{r}
str(hclust_ward)
```

We can then plot the tree by using *plot()* of R Graphics as shown in the code chunk below.

**cex**. A numerical value giving the amount by which plotting text and symbols should be magnified relative to the default. cex = 0.6 means to scale down to 60% (when knitting to html) to prevent overlapping Base R plot will know to plot a dendrogram without specifying the type, because its a hclust object?

```{r}
plot(hclust_ward, cex = 0.6)
```

### 5.7.8 Selecting the optimal clustering algorithm

One of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use [*agnes()*](https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/agnes) function of [**cluster**](https://cran.r-project.org/web/packages/cluster/) package. It functions like *hclus()*, however, with the *agnes()* function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

The code chunk below will be used to compute the agglomerative coefficients (ac) of all hierarchical clustering algorithms.

Breaking down the big formula into smaller

```{r}
attributes(agnes(shan_ict, method = 'average'))
agnes(shan_ict, method = 'average')$ac
```

Define a function ac, this function input is m, which inputs various clustering algorithms into the function agnes(). Next, use the map_dbl function to map the the **m** into **agnes()**

**\$ac means to call the 'agglomerative coeeficients' component of agnes function (see above)**

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

agnes() calculates an index to measure level of homogeneity between different methods of hclustering.

With reference to the output above, we can see that Ward's method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward's method will be used.

### 5.7.9 Determining Optimal Clusters

Another technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.

There are [three](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/) commonly used methods to determine the optimal clusters, they are:

-   [Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering))

-   [Average Silhouette Method](https://www.sciencedirect.com/science/article/pii/0377042787901257?via%3Dihub)

-   [Gap Statistic Method](https://statweb.stanford.edu/~gwalther/gap)

#### 5.7.9.1 Gap Statistic Method

The [**gap statistic**](http://www.web.stanford.edu/~hastie/Papers/gap.pdf) compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that **maximize** the gap statistic (i.e., that yields the **largest gap statistic**). This means that the clustering structure is far away from the random uniform distribution of points.

To compute the gap statistic, [*clusGap()*](https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/clusGap) of [**cluster**](https://cran.r-project.org/web/packages/cluster/) package will be used.

K.max - maximum number of clusters to consider, minimum 2

B = integer, number of Monte Carlo ('bootstrap') samples

nstart \<- isnt this only applicable to Kmeans clustering, is it about initialising a centroid?

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict,
                    FUN = hcut,
                    nstart = 25,
                    K.max= 10, 
                    B= 50)

#print the results
print(gap_stat, method = 'firstmax')
```

Also note that the [*hcut*](https://rpkgs.datanovia.com/factoextra/reference/hcut.html) function used is from [**factoextra**](https://rpkgs.datanovia.com/factoextra/) package.

Next, we can visualise the plot by using [*fviz_gap_stat()*](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html) of [**factoextra**](https://rpkgs.datanovia.com/factoextra/) package.

```{r}
fviz_gap_stat(gap_stat)
```

With reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

**Note:** In addition to these commonly used approaches, the [NbClust](https://cran.r-project.org/web/packages/NbClust/) package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.

Find the optimal number of clusters (elbow, gap or silhouette methods): https://rstudio-pubs-static.s3.amazonaws.com/708276_a1cfeb43954b4206b1cededbe995a47f.html

### 5.7.10 Interpreting the dendrograms

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.

It's also possible to draw the dendrogram with a border around the selected clusters by using [*rect.hclust()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/rect.hclust.html) of R stats. The argument *border* is used to specify the border colors for the rectangles.

k - refers to number of clusters \<- use a line that cuts horizontally to get 6 clusters

border = vector with border colours for the rectangles. Can also put 'red' or 'green'...

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward,
            k=6,
            border = 2:5)
```

### 5.7.11 Visually-driven hierarchical clustering analysis

In this section, we will learn how to perform visually-driven hierarchical clustering analysis by using [*heatmaply*](https://cran.r-project.org/web/packages/heatmaply/) package.

With **heatmaply**, we are able to build both highly interactive cluster heatmap or static cluster heatmap.

#### 5.7.11.1 Transforming the data frame into a matrix

The data was loaded into a **data frame**, but it has to be a **data matrix** to make the heatmap.

```{r}
class(shan_ict)
```

```{r}
shan_ict_mat <- as.matrix(shan_ict)
class(shan_ict_mat)
```

#### 5.7.11.2 Plotting interactive cluster heatmap using *heatmaply()*

In the code chunk below, the [*heatmaply()*](https://talgalili.github.io/heatmaply/reference/heatmaply.html) of [heatmaply](https://talgalili.github.io/heatmaply/) package is used to build an interactive cluster heatmap.

```{r}
# first perform min-max normalisation
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = 'euclidean',
          hclust_method = 'ward.D',
          seriate = 'OLO',
          colors= Blues,
          k_row = 6,
          margins = c(NA, 200, 60, NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main = 'Geographic Segmentation of Shan State by ICT indicators',
          xlab= 'ICT indicators',
          ylab = 'Townships of Shan State'
)
          

```

### 5.7.12 Mapping the clusters formed

With closed examination of the dendragram above, we have decided to retain six clusters.

[*cutree()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html) of R Base will be used in the code chunk below to derive a 6-cluster model.\\

Each region will be assigned an integer representing their cluster number.

```{r}
cutree(hclust_ward, k=6)
class(cutree(hclust_ward, k=6))
```

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

Yixin checking data format. Did not use str()

```{r}
attributes(groups)
class(groups)
typeof(attributes(groups)$level)
```

The output ***groups*** is a *list* object.

In order to visualise the clusters using **qtm()**, the *groups* object need to be appended onto *shan_sf* simple feature object.

The code chunk below form the join in three steps:

-   the *groups* list object will be converted into a matrix;

-   *cbind()* is used to append *groups* matrix onto shan_sf to produce an output simple feature object called `shan_sf_cluster`; and

-   *rename* of **dplyr** package is used to rename *as.matrix.groups* field as *CLUSTER*.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups))
```

Rename 'as.matrix.group' column to CLUSTER

```{r}
shan_sf_cluster <- shan_sf_cluster %>%  rename('CLUSTER' = 'as.matrix.groups.')
```

Next, *qtm()* of **tmap** package is used to plot the choropleth map showing the cluster formed.

```{r}
qtm(shan_sf_cluster, 'CLUSTER')
```

The choropleth map above reveals the clusters are very fragmented, the clusters are formed by attributes relationship only and not by spatial relationship. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.

## 5.8 Spatially Constrained Clustering - SKATER approach

In this section, you will learn how to derive spatially constrained cluster by using [*skater()*](https://r-spatial.github.io/spdep/reference/skater.html) method of [**spdep**](https://r-spatial.github.io/spdep/) package.

### 5.8.1 Converting into SpatialPolygonsDataFrame (required by SKATER 2006 function)

First, we need to convert `shan_sf` from sf into SpatialPolygonsDataFrame (sp). This is because SKATER function only support **sp** objects such as SpatialPolygonDataFrame.

The code chunk below uses [*as_Spatial()*](https://r-spatial.github.io/sf/reference/coerce-methods.html) of **sf** package to convert *shan_sf* into a SpatialPolygonDataFrame called *shan_sp*.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### 5.8.2 Computing Neighbour List

Next, [poly2nb()](https://r-spatial.github.io/spdep/reference/poly2nb.html) of **spdep** package will be used to compute the neighbours list from polygon list. SKATER is based on contiguity concept, so use ply2nb() here.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

```{r}
str(shan.nb)
```

With shan_sp,

Firstly, plot boundaries on the bottom-most layer first.

Secondly, plot **neighbours list object** on top of boundaries map. The **centroid coordinates** of each polygons/ neighbours can be extracted using coordinates(shan_sp) on the sp object. Set the color to blue and specify add=TRUE to plot the network on top of the boundaries.

```{r}
head(coordinates(shan_sp))
```

```{r}
plot(shan_sp, 
     border=grey(.5))

plot(shan.nb, coordinates(shan_sp),
     col='blue',
     add=TRUE)
```

Note that if we plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot at the bottom-most layer. In this example, because the boundary map extends further (is bigger?) than the graph, we plot it first at bottom layer.

### 5.8.3 Computing minimum spanning tree

#### 5.8.3.1 Calculating edge costs

Next, [*nbcosts()*](https://r-spatial.github.io/spdep/reference/nbcosts.html) of **spdep** package is used to compute the cost of each edge / connection. It is the distance between each nodes. This function compute this distance using a data.frame with observations ( 5 ict variables TV, radio etc..) vector in each node. lcost is a 'nbdist' class object.

The code chunk below is used to compute the cost of each edge. Recall that shan_ict is a df containing 5 variables. shan.nb is a neighbour list object.

```{r}
lcost <- nbcosts(shan.nb, shan_ict)
```

```{r}
class(lcost)
str(lcost)
```

If I have four neighbours, then I would have four edge costs.

For each observation, this gives the pairwise **dissimilarity** between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.

Next, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the [**neighbour list object**]{.underline} to a [**list weights object**]{.underline} by specifying the ***lcost*** as the weights.

Note that we specify the *style* as **B** to make sure the cost values are not row-standardised.

```{r}
#?nb2listw
shan.w <- nb2listw(shan.nb,
                   glist=lcost,
                   style ='B')
summary(shan.w)
```

shan.w is a weights list object (listw)

```{r}
str(shan.w)
```

### 5.8.4 Computing minimum spanning tree (Each polygon has exactly one neighbour)

The minimum spanning tree is computed by mean of the [*mstree()*](https://r-spatial.github.io/spdep/reference/mstree.html) of **spdep** package as shown in the code chunk below.

```{r}
shan.mst <- mstree(shan.w)
```

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```

Note that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.

Yi xin: We can display the content of *shan.mst* by using *head()* as shown in the code chunk below. There are 54 edges in total, but we display only five here. The first node polygon 31 and the minimum cost is 229.446 to travel to polygon 25. The tree path is created by considering the minimum cost path from all available paths. The cost is calculated using the neighbours list first, then ICT variables . Each polygon has exactly one neighbour in the Minimum Spanning Tree. This is 100% geospatial relationship as consider neighbour list first.

shan_sf -\> shan_sp -\> shan.nb -\> lcost = nbcosts(shan.nb, shan_ict) -\> shan.w= nb2listw(shan.nb, glist=lcost, style ='B') -\> shan.mst \<- mstree(shan.w)

```{r}
head(shan.mst)
```

The plot method for the MST include a way to show the **observation numbers** of the **nodes** (polygon ID) in addition to the **edge.** As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just **one edge** connecting **each** of the **nodes**, while passing through **all** the nodes.

```{r}
plot(shan_sp,
     border = grey(.5))

plot.mst(shan.mst,
         coords=coordinates(shan_sp),         
         col='blue',
         cex.lab= 0.7,
         cex.circles = 0.005,
         add=TRUE)
```

### 5.8.5 Computing spatially constrained clusters using SKATER method

The code chunk below compute the spatially constrained cluster using [*skater()*](https://r-spatial.github.io/spdep/reference/skater.html) of **spdep** package.

shan.mst\[, 1:2\] refer to the first two components of shan.mst (see above). They refer to the edges literally, not the costs.

By cutting the MST five times, we get six clusters. We have to perform hierarchical clustering first; else do not know optimal clusters is 6.

```{r}
clust6 <- spdep::skater(edges = shan.mst[, 1:2],
                        data = shan_ict,
                        method = 'euclidean',
                        ncuts = 5)
```

The *skater()* takes **three mandatory** arguments:

\- the first two columns of the MST matrix (i.e. not the cost),

\- the data matrix (to update the costs as units are being grouped), and

 - the number of cuts. Note: It is set to **one less than the number of clusters**. So, the value specified is **not** the number of clusters, but the number of cuts in the graph, one less than the number of clusters.

The result of the *skater()* is an object of class **skater**. We can examine its contents by using the code chunk below.

```{r}
str(clust6)
```

### 5.8.6 Visualising the clusters in choropleth map
